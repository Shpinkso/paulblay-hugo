<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>AI on Paul Blay</title><link>https://shpinkso.github.io/paulblay-hugo/previews/main/tags/ai/</link><description>Recent content in AI on Paul Blay</description><generator>Hugo -- gohugo.io</generator><language>en-gb</language><lastBuildDate>Wed, 15 Oct 2025 11:37:12 +0100</lastBuildDate><atom:link href="https://shpinkso.github.io/paulblay-hugo/previews/main/tags/ai/index.xml" rel="self" type="application/rss+xml"/><item><title>Painful Vibe coding with Codex</title><link>https://shpinkso.github.io/paulblay-hugo/previews/main/blog/2025/10/15/trying_out_codex/</link><pubDate>Wed, 15 Oct 2025 11:37:12 +0100</pubDate><guid>https://shpinkso.github.io/paulblay-hugo/previews/main/blog/2025/10/15/trying_out_codex/</guid><description>Use of LLMs I&amp;rsquo;ve been using LLMs for a while, primarily as a buddy that can help me with ideation or noticing gaps in my thinking.
I rarely use the LLM output directly because it never feels complete or authentic. For written word, I find it can generate pretty generic wording with little impact or personality.
Anything I do take directly, I&amp;rsquo;ll rework pretty heavily - and it&amp;rsquo;s just a judgement of whether doing the rework is more efficient than just starting from scratch.</description></item><item><title>A Policy for AI-enhanced software development</title><link>https://shpinkso.github.io/paulblay-hugo/previews/main/blog/2025/10/07/ai_in_software_development/</link><pubDate>Tue, 07 Oct 2025 08:52:20 +0100</pubDate><guid>https://shpinkso.github.io/paulblay-hugo/previews/main/blog/2025/10/07/ai_in_software_development/</guid><description>The DORA research group at Google recently came out with an AI capabilities model and capability number 1 is to have a &amp;ldquo;Clear and communicated AI stance&amp;rdquo;. Given my experience so far talking to other leaders and on the ground, here&amp;rsquo;s my proposal for what an effective AI stance could look like:
Core Philosophy LLMs should amplify engineering cognition, not automate it
This means&amp;hellip;
Human Ownership: Engineers must remain the authors and owners of production logic.</description></item></channel></rss>